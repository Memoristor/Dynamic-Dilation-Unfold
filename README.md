# Dynamic Dilation Unfold

<div align="center">

[English](#english) | [ä¸­æ–‡](#chinese)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 1.8+](https://img.shields.io/badge/pytorch-1.8+-red.svg)](https://pytorch.org/)
[![CUDA](https://img.shields.io/badge/cuda-10.2+-green.svg)](https://developer.nvidia.com/cuda-toolkit)

</div>

---

<a name="english"></a>
## English Documentation

### ğŸ“– Overview

**Dynamic Dilation Unfold** is a PyTorch extension that implements spatially-varying dilation rates for the unfold (im2col) operation with high-performance CUDA acceleration. Unlike standard `F.unfold` which uses a fixed dilation rate across the entire feature map, this implementation allows each spatial position to have its own dilation rate, enabling more flexible and adaptive feature extraction.

This is inspired by Deformable Convolution (DCNv2), but focuses specifically on the unfold operation, making it useful for:
- Attention mechanisms with adaptive receptive fields
- Multi-scale feature extraction
- Deformable sampling in transformers
- Custom pooling operations with varying receptive fields

### âœ¨ Key Features

- **ğŸ”„ Spatially-Varying Dilation**: Each output position can have different dilation rates
- **âš¡ CUDA Acceleration**: Optimized CUDA kernels for maximum performance
- **ğŸ“ Full Autograd Support**: Differentiable with respect to both input and dilation_map
- **ğŸ”§ Easy Integration**: Drop-in replacement compatible with `torch.nn.Unfold` API
- **ğŸ’ª Mixed Precision**: Supports FP16, FP32, and FP64
- **ğŸ¯ Bilinear Interpolation**: Smooth sampling for sub-pixel positions
- **ğŸ“¦ Simple Installation**: One-line pip installation

### ğŸš€ Installation

#### Prerequisites

```bash
# Check your environment
python --version  # >= 3.7
python -c "import torch; print(torch.__version__)"  # >= 1.8.0
python -c "import torch; print(torch.version.cuda)"  # Check CUDA version
nvcc --version  # Should match PyTorch CUDA version
```

#### Install from Source

```bash
git clone https://github.com/yourusername/dynamic_dilation_unfold.git
cd dynamic_dilation_unfold
pip install -e .
```

If you encounter compilation errors:

```bash
# Clean previous builds
rm -rf build/ *.egg-info
python setup.py clean --all

# Reinstall with verbose output
pip install -e . -v
```

### ğŸ“š Quick Start

#### Basic Usage

```python
import torch
from dynamic_dilation_unfold import dynamic_dilation_unfold

# Create input tensor (B, C, H, W)
x = torch.randn(2, 3, 32, 32).cuda()

# Create dilation map (B, 1, H_out, W_out)
# Each value represents the dilation rate at that position
dilation_map = torch.ones(2, 1, 32, 32).cuda()

# Apply dynamic dilation unfold
output = dynamic_dilation_unfold(
    input=x,
    kernel_size=3,
    dilation_map=dilation_map,
    stride=1,
    padding=1,
    dilation=1  # Base dilation multiplier
)

print(f"Input shape:  {x.shape}")           # [2, 3, 32, 32]
print(f"Output shape: {output.shape}")      # [2, 27, 1024]
# Output: [B, C*kH*kW, H_out*W_out]
```

#### Module Interface

```python
from dynamic_dilation_unfold import DynamicDilationUnfold

# Create reusable module
unfold = DynamicDilationUnfold(
    kernel_size=3,
    stride=1,
    padding=1,
    dilation=1
).cuda()

# Use in forward pass
output = unfold(x, dilation_map)
```

#### Advanced: Spatially-Varying Dilation

```python
import torch
import matplotlib.pyplot as plt

# Create input
x = torch.randn(1, 3, 64, 64).cuda()

# Create spatially-varying dilation map
dilation_map = torch.ones(1, 1, 64, 64).cuda()

# Top-left: small receptive field (dilation=0.5)
dilation_map[:, :, :32, :32] = 0.5

# Top-right: medium receptive field (dilation=1.0)
dilation_map[:, :, :32, 32:] = 1.0

# Bottom-left: large receptive field (dilation=2.0)
dilation_map[:, :, 32:, :32] = 2.0

# Bottom-right: very large receptive field (dilation=3.0)
dilation_map[:, :, 32:, 32:] = 3.0

# Apply unfold
output = dynamic_dilation_unfold(x, kernel_size=3, dilation_map=dilation_map)

# Visualize dilation map
plt.imshow(dilation_map[0, 0].cpu(), cmap='viridis')
plt.colorbar(label='Dilation Rate')
plt.title('Spatially-Varying Dilation Map')
plt.show()
```

### ğŸ”§ API Reference

#### Function API

```python
dynamic_dilation_unfold(
    input: torch.Tensor,
    kernel_size: Union[int, Tuple[int, int]],
    dilation_map: torch.Tensor,
    stride: Union[int, Tuple[int, int]] = 1,
    padding: Union[int, Tuple[int, int]] = 0,
    dilation: Union[int, Tuple[int, int]] = 1
) -> torch.Tensor
```

**Parameters:**

- **input** (*torch.Tensor*): Input tensor of shape `(B, C, H, W)`
- **kernel_size** (*int or tuple*): Size of the sliding window. Can be a single int or tuple `(kH, kW)`
- **dilation_map** (*torch.Tensor*): Dilation map of shape `(B, 1, H_out, W_out)`. Each value >= 0 indicates the dilation rate at that spatial position
- **stride** (*int or tuple, optional*): Stride of the sliding window. Default: 1
- **padding** (*int or tuple, optional*): Implicit zero padding. Default: 0
- **dilation** (*int or tuple, optional*): Base dilation multiplier. Default: 1

**Returns:**

- **output** (*torch.Tensor*): Output tensor of shape `(B, C * kH * kW, H_out * W_out)`

**Output Shape Calculation:**

```python
H_out = (H + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1
W_out = (W + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1
```

#### Module API

```python
class DynamicDilationUnfold(nn.Module):
    def __init__(
        self,
        kernel_size: Union[int, Tuple[int, int]],
        stride: Union[int, Tuple[int, int]] = 1,
        padding: Union[int, Tuple[int, int]] = 0,
        dilation: Union[int, Tuple[int, int]] = 1
    )
    
    def forward(
        self,
        input: torch.Tensor,
        dilation_map: torch.Tensor
    ) -> torch.Tensor
```

### ğŸ’¡ Usage Example: Gradient Flow Analysis

```python
import torch
from dynamic_dilation_unfold import dynamic_dilation_unfold

# Enable gradient tracking
x = torch.randn(2, 3, 16, 16, requires_grad=True).cuda()
dilation_map = torch.ones(2, 1, 16, 16, requires_grad=True).cuda() * 1.5

# Forward pass
output = dynamic_dilation_unfold(x, kernel_size=3, dilation_map=dilation_map, padding=1)

# Backward pass
loss = output.mean()
loss.backward()

# Analyze gradients
print("Input gradient:")
print(f"  Shape: {x.grad.shape}")
print(f"  Mean: {x.grad.mean().item():.6f}")
print(f"  Std: {x.grad.std().item():.6f}")
print(f"  Has NaN: {torch.isnan(x.grad).any()}")

print("\nDilation map gradient:")
print(f"  Shape: {dilation_map.grad.shape}")
print(f"  Mean: {dilation_map.grad.mean().item():.6f}")
print(f"  Std: {dilation_map.grad.std().item():.6f}")
print(f"  Has NaN: {torch.isnan(dilation_map.grad).any()}")
```

### ğŸ§ª Testing

Run the comprehensive test suite:

```bash
cd tests
python test_dynamic_unfold.py
```

**Test Coverage:**

- âœ… Basic forward pass
- âœ… Gradient computation (input & dilation_map)
- âœ… Numerical gradient verification
- âœ… Different dilation values
- âœ… Spatially-varying dilation
- âœ… Module interface
- âœ… Edge cases (zero/large dilations)
- âœ… Mixed precision (FP16/FP32/FP64)

**Sample Output:**

```
======================================================================
Running Dynamic Dilation Unfold Tests
======================================================================

=== Test Basic Forward ===
Output shape: torch.Size([2, 27, 64])
âœ“ Basic forward test passed

=== Test Gradient Input ===
Input gradient shape: torch.Size([2, 2, 6, 6])
Input gradient mean: 0.150234
Input gradient std: 0.489123
âœ“ Input gradient test passed

=== Test Numerical Gradient ===
Max relative error: 0.000023
Mean relative error: 0.000008
âœ“ Numerical gradient test passed

======================================================================
âœ“ All tests passed!
======================================================================
```

### ğŸ“Š Performance Benchmark

#### Speed Comparison

```python
import torch
import time
from dynamic_dilation_unfold import dynamic_dilation_unfold

def benchmark(func, *args, warmup=10, iterations=100):
    # Warmup
    for _ in range(warmup):
        func(*args)
    
    torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(iterations):
        func(*args)
    
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    return elapsed / iterations * 1000  # ms

# Setup
B, C, H, W = 4, 64, 128, 128
x = torch.randn(B, C, H, W).cuda()
dilation_map = torch.ones(B, 1, H, W).cuda()

# Benchmark
time_dynamic = benchmark(lambda: dynamic_dilation_unfold(x, 3, dilation_map, padding=1))

print(f"Dynamic Dilation Unfold: {time_dynamic:.2f} ms")
print(f"Input shape: {x.shape}")
```

#### Memory Usage

```python
import torch
from dynamic_dilation_unfold import dynamic_dilation_unfold

torch.cuda.reset_peak_memory_stats()

x = torch.randn(4, 64, 128, 128).cuda()
dilation_map = torch.ones(4, 1, 128, 128).cuda()

output = dynamic_dilation_unfold(x, kernel_size=3, dilation_map=dilation_map, padding=1)

memory_mb = torch.cuda.max_memory_allocated() / 1024 / 1024
print(f"Peak memory usage: {memory_mb:.2f} MB")
```

### ğŸ”¬ How It Works

#### Algorithm Overview

For each output position `(b, c, h_out, w_out)`:

1. **Read Dilation Rate**: Get `d = dilation_map[b, 0, h_out, w_out]`

2. **Compute Sampling Positions**: For each kernel position `(kh, kw)`:
   ```python
   h_in = h_out * stride_h - pad_h + (kh - (K-1)/2) * base_dilation * d
   w_in = w_out * stride_w - pad_w + (kw - (K-1)/2) * base_dilation * d
   ```

3. **Bilinear Interpolation**: Sample input at `(h_in, w_in)` using bilinear interpolation

4. **Store Result**: Place sampled value in output tensor

#### Backward Pass

**Gradient w.r.t. Input:**
- Distribute output gradients to the 4 neighboring pixels used in bilinear interpolation
- Use atomic operations for thread-safe accumulation

**Gradient w.r.t. Dilation Map:**
- Compute how the sampling position changes with dilation
- Apply chain rule through bilinear interpolation
- Accumulate across all kernel positions and channels

#### Illustration

```
Standard Unfold (dilation=1):          Dynamic Dilation (dilation_map):
                                       
    [Â·][Â·][Â·]                              [Â·]    [Â·]    [Â·]
    [Â·][X][Â·]      kernel_size=3           
    [Â·][Â·][Â·]                              [Â·]    [X]    [Â·]      dilation=2.0
                                           
                                           [Â·]    [Â·]    [Â·]


    [Â·][Â·][Â·]                              [Â·]      [Â·]      [Â·]
    [Â·][X][Â·]      kernel_size=3           
    [Â·][Â·][Â·]                              [Â·]      [X]      [Â·]  dilation=3.0
                                           
                                           [Â·]      [Â·]      [Â·]
```

### ğŸ¤” FAQ

**Q: What's the difference from standard `F.unfold`?**

A: Standard `F.unfold` uses a fixed dilation rate for all positions. Dynamic Dilation Unfold allows each spatial location to have its own dilation rate specified by `dilation_map`.

**Q: Can I use this with convolutional layers?**

A: This is specifically for the unfold operation. For dynamic convolutions, consider using Deformable Convolution (DCNv2). However, you can combine this unfold with manual matrix multiplication to achieve similar effects.

**Q: What happens if dilation is 0?**

A: When dilation is 0, all kernel positions sample from the center point. This creates a "collapsed" receptive field, which might be useful for certain applications.

**Q: Is this differentiable?**

A: Yes! Both the input and dilation_map have full gradient support through bilinear interpolation.

**Q: Can I use fractional dilation values?**

A: Yes! The dilation_map supports any non-negative floating-point values, enabling smooth, continuous control over receptive field sizes.

**Q: What's the computational overhead?**

A: The main overhead comes from bilinear interpolation (4 samples per kernel position) and atomic operations in the backward pass. Typically 2-3x slower than standard unfold for similar configurations.

**Q: Does it support mixed precision training?**

A: Yes! The implementation supports FP16 (half), FP32 (float), and FP64 (double) precision.

### ğŸ“„ License

This project is licensed under the MIT License.

### ğŸ™ Acknowledgments

This project is inspired by:
- [Deformable Convolution V2](https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch) by Chengdazhi
- [DCNv2](https://github.com/CharlesShang/DCNv2) by CharlesShang

---

<a name="chinese"></a>
## ä¸­æ–‡æ–‡æ¡£

### ğŸ“– æ¦‚è¿°

**Dynamic Dilation Unfoldï¼ˆåŠ¨æ€æ‰©å¼ å±•å¼€ï¼‰** æ˜¯ä¸€ä¸ªPyTorchæ‰©å±•ï¼Œå®ç°äº†å…·æœ‰ç©ºé—´å¯å˜æ‰©å¼ ç‡çš„unfoldï¼ˆim2colï¼‰æ“ä½œï¼Œå¹¶æä¾›é«˜æ€§èƒ½çš„CUDAåŠ é€Ÿã€‚ä¸æ ‡å‡†çš„`F.unfold`åœ¨æ•´ä¸ªç‰¹å¾å›¾ä¸Šä½¿ç”¨å›ºå®šæ‰©å¼ ç‡ä¸åŒï¼Œè¯¥å®ç°å…è®¸æ¯ä¸ªç©ºé—´ä½ç½®æ‹¥æœ‰è‡ªå·±çš„æ‰©å¼ ç‡ï¼Œä»è€Œå®ç°æ›´çµæ´»å’Œè‡ªé€‚åº”çš„ç‰¹å¾æå–ã€‚

è¯¥é¡¹ç›®å—åˆ°Deformable Convolution (DCNv2)çš„å¯å‘ï¼Œä½†ä¸“æ³¨äºunfoldæ“ä½œï¼Œé€‚ç”¨äºï¼š
- å…·æœ‰è‡ªé€‚åº”æ„Ÿå—é‡çš„æ³¨æ„åŠ›æœºåˆ¶
- å¤šå°ºåº¦ç‰¹å¾æå–
- Transformerä¸­çš„å¯å˜å½¢é‡‡æ ·
- å…·æœ‰å¯å˜æ„Ÿå—é‡çš„è‡ªå®šä¹‰æ± åŒ–æ“ä½œ

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- **ğŸ”„ ç©ºé—´å¯å˜æ‰©å¼ ç‡**ï¼šæ¯ä¸ªè¾“å‡ºä½ç½®å¯ä»¥æœ‰ä¸åŒçš„æ‰©å¼ ç‡
- **âš¡ CUDAåŠ é€Ÿ**ï¼šä¼˜åŒ–çš„CUDAå†…æ ¸ï¼Œæ€§èƒ½å“è¶Š
- **ğŸ“ å®Œæ•´çš„è‡ªåŠ¨å¾®åˆ†æ”¯æŒ**ï¼šå¯¹è¾“å…¥å’Œæ‰©å¼ å›¾éƒ½å¯å¾®åˆ†
- **ğŸ”§ æ˜“äºé›†æˆ**ï¼šAPIå…¼å®¹`torch.nn.Unfold`ï¼Œå¯ç›´æ¥æ›¿æ¢
- **ğŸ’ª æ··åˆç²¾åº¦**ï¼šæ”¯æŒFP16ã€FP32å’ŒFP64
- **ğŸ¯ åŒçº¿æ€§æ’å€¼**ï¼šäºšåƒç´ ä½ç½®çš„å¹³æ»‘é‡‡æ ·
- **ğŸ“¦ ç®€å•å®‰è£…**ï¼šä¸€è¡Œå‘½ä»¤å®Œæˆå®‰è£…

### ğŸš€ å®‰è£…

#### ç¯å¢ƒè¦æ±‚

```bash
# æ£€æŸ¥ç¯å¢ƒ
python --version  # >= 3.7
python -c "import torch; print(torch.__version__)"  # >= 1.8.0
python -c "import torch; print(torch.version.cuda)"  # æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version  # åº”ä¸PyTorchçš„CUDAç‰ˆæœ¬åŒ¹é…
```

#### ä»æºç å®‰è£…

```bash
git clone https://github.com/yourusername/dynamic_dilation_unfold.git
cd dynamic_dilation_unfold
pip install -e .
```

å¦‚æœé‡åˆ°ç¼–è¯‘é”™è¯¯ï¼š

```bash
# æ¸…ç†ä¹‹å‰çš„æ„å»º
rm -rf build/ *.egg-info
python setup.py clean --all

# é‡æ–°å®‰è£…ï¼ˆæ˜¾ç¤ºè¯¦ç»†è¾“å‡ºï¼‰
pip install -e . -v
```

### ğŸ“š å¿«é€Ÿå¼€å§‹

#### åŸºç¡€ç”¨æ³•

```python
import torch
from dynamic_dilation_unfold import dynamic_dilation_unfold

# åˆ›å»ºè¾“å…¥å¼ é‡ (B, C, H, W)
x = torch.randn(2, 3, 32, 32).cuda()

# åˆ›å»ºæ‰©å¼ å›¾ (B, 1, H_out, W_out)
# æ¯ä¸ªå€¼ä»£è¡¨è¯¥ä½ç½®çš„æ‰©å¼ ç‡
dilation_map = torch.ones(2, 1, 32, 32).cuda()

# åº”ç”¨åŠ¨æ€æ‰©å¼ unfold
output = dynamic_dilation_unfold(
    input=x,
    kernel_size=3,
    dilation_map=dilation_map,
    stride=1,
    padding=1,
    dilation=1  # åŸºç¡€æ‰©å¼ ç³»æ•°
)

print(f"è¾“å…¥å½¢çŠ¶:  {x.shape}")           # [2, 3, 32, 32]
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")      # [2, 27, 1024]
# è¾“å‡º: [B, C*kH*kW, H_out*W_out]
```

#### æ¨¡å—æ¥å£

```python
from dynamic_dilation_unfold import DynamicDilationUnfold

# åˆ›å»ºå¯é‡ç”¨çš„æ¨¡å—
unfold = DynamicDilationUnfold(
    kernel_size=3,
    stride=1,
    padding=1,
    dilation=1
).cuda()

# åœ¨å‰å‘ä¼ æ’­ä¸­ä½¿ç”¨
output = unfold(x, dilation_map)
```

#### è¿›é˜¶ï¼šç©ºé—´å¯å˜æ‰©å¼ 

```python
import torch
import matplotlib.pyplot as plt

# åˆ›å»ºè¾“å…¥
x = torch.randn(1, 3, 64, 64).cuda()

# åˆ›å»ºç©ºé—´å¯å˜çš„æ‰©å¼ å›¾
dilation_map = torch.ones(1, 1, 64, 64).cuda()

# å·¦ä¸Šï¼šå°æ„Ÿå—é‡ (æ‰©å¼ ç‡=0.5)
dilation_map[:, :, :32, :32] = 0.5

# å³ä¸Šï¼šä¸­ç­‰æ„Ÿå—é‡ (æ‰©å¼ ç‡=1.0)
dilation_map[:, :, :32, 32:] = 1.0

# å·¦ä¸‹ï¼šå¤§æ„Ÿå—é‡ (æ‰©å¼ ç‡=2.0)
dilation_map[:, :, 32:, :32] = 2.0

# å³ä¸‹ï¼šè¶…å¤§æ„Ÿå—é‡ (æ‰©å¼ ç‡=3.0)
dilation_map[:, :, 32:, 32:] = 3.0

# åº”ç”¨unfold
output = dynamic_dilation_unfold(x, kernel_size=3, dilation_map=dilation_map)

# å¯è§†åŒ–æ‰©å¼ å›¾
plt.imshow(dilation_map[0, 0].cpu(), cmap='viridis')
plt.colorbar(label='æ‰©å¼ ç‡')
plt.title('ç©ºé—´å¯å˜æ‰©å¼ å›¾')
plt.show()
```

### ğŸ”§ APIå‚è€ƒ

#### å‡½æ•°API

```python
dynamic_dilation_unfold(
    input: torch.Tensor,
    kernel_size: Union[int, Tuple[int, int]],
    dilation_map: torch.Tensor,
    stride: Union[int, Tuple[int, int]] = 1,
    padding: Union[int, Tuple[int, int]] = 0,
    dilation: Union[int, Tuple[int, int]] = 1
) -> torch.Tensor
```

**å‚æ•°ï¼š**

- **input** (*torch.Tensor*): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º`(B, C, H, W)`
- **kernel_size** (*int æˆ– tuple*): æ»‘åŠ¨çª—å£çš„å¤§å°ï¼Œå¯ä»¥æ˜¯å•ä¸ªæ•´æ•°æˆ–å…ƒç»„`(kH, kW)`
- **dilation_map** (*torch.Tensor*): æ‰©å¼ å›¾ï¼Œå½¢çŠ¶ä¸º`(B, 1, H_out, W_out)`ã€‚æ¯ä¸ªå€¼>=0ï¼Œè¡¨ç¤ºè¯¥ç©ºé—´ä½ç½®çš„æ‰©å¼ ç‡
- **stride** (*int æˆ– tuple, å¯é€‰*): æ»‘åŠ¨çª—å£çš„æ­¥é•¿ã€‚é»˜è®¤å€¼: 1
- **padding** (*int æˆ– tuple, å¯é€‰*): éšå¼é›¶å¡«å……ã€‚é»˜è®¤å€¼: 0
- **dilation** (*int æˆ– tuple, å¯é€‰*): åŸºç¡€æ‰©å¼ ç³»æ•°ã€‚é»˜è®¤å€¼: 1

**è¿”å›å€¼ï¼š**

- **output** (*torch.Tensor*): è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º`(B, C * kH * kW, H_out * W_out)`

**è¾“å‡ºå½¢çŠ¶è®¡ç®—ï¼š**

```python
H_out = (H + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1
W_out = (W + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1
```

#### æ¨¡å—API

```python
class DynamicDilationUnfold(nn.Module):
    def __init__(
        self,
        kernel_size: Union[int, Tuple[int, int]],
        stride: Union[int, Tuple[int, int]] = 1,
        padding: Union[int, Tuple[int, int]] = 0,
        dilation: Union[int, Tuple[int, int]] = 1
    )
    
    def forward(
        self,
        input: torch.Tensor,
        dilation_map: torch.Tensor
    ) -> torch.Tensor
```

### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹ï¼šæ¢¯åº¦æµåˆ†æ

```python
import torch
from dynamic_dilation_unfold import dynamic_dilation_unfold

# å¯ç”¨æ¢¯åº¦è·Ÿè¸ª
x = torch.randn(2, 3, 16, 16, requires_grad=True).cuda()
dilation_map = torch.ones(2, 1, 16, 16, requires_grad=True).cuda() * 1.5

# å‰å‘ä¼ æ’­
output = dynamic_dilation_unfold(x, kernel_size=3, dilation_map=dilation_map, padding=1)

# åå‘ä¼ æ’­
loss = output.mean()
loss.backward()

# åˆ†ææ¢¯åº¦
print("è¾“å…¥æ¢¯åº¦:")
print(f"  å½¢çŠ¶: {x.grad.shape}")
print(f"  å¹³å‡å€¼: {x.grad.mean().item():.6f}")
print(f"  æ ‡å‡†å·®: {x.grad.std().item():.6f}")
print(f"  æœ‰NaN: {torch.isnan(x.grad).any()}")

print("\næ‰©å¼ å›¾æ¢¯åº¦:")
print(f"  å½¢çŠ¶: {dilation_map.grad.shape}")
print(f"  å¹³å‡å€¼: {dilation_map.grad.mean().item():.6f}")
print(f"  æ ‡å‡†å·®: {dilation_map.grad.std().item():.6f}")
print(f"  æœ‰NaN: {torch.isnan(dilation_map.grad).any()}")
```

### ğŸ§ª æµ‹è¯•

è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼š

```bash
cd tests
python test_dynamic_unfold.py
```

**æµ‹è¯•è¦†ç›–ï¼š**

- âœ… åŸºæœ¬å‰å‘ä¼ æ’­
- âœ… æ¢¯åº¦è®¡ç®—ï¼ˆè¾“å…¥å’Œæ‰©å¼ å›¾ï¼‰
- âœ… æ•°å€¼æ¢¯åº¦éªŒè¯
- âœ… ä¸åŒæ‰©å¼ å€¼
- âœ… ç©ºé—´å¯å˜æ‰©å¼ 
- âœ… æ¨¡å—æ¥å£
- âœ… è¾¹ç•Œæƒ…å†µï¼ˆé›¶/å¤§æ‰©å¼ ç‡ï¼‰
- âœ… æ··åˆç²¾åº¦ï¼ˆFP16/FP32/FP64ï¼‰

**ç¤ºä¾‹è¾“å‡ºï¼š**

```
======================================================================
è¿è¡ŒåŠ¨æ€æ‰©å¼ Unfoldæµ‹è¯•
======================================================================

=== æµ‹è¯•åŸºæœ¬å‰å‘ä¼ æ’­ ===
è¾“å‡ºå½¢çŠ¶: torch.Size([2, 27, 64])
âœ“ åŸºæœ¬å‰å‘æµ‹è¯•é€šè¿‡

=== æµ‹è¯•è¾“å…¥æ¢¯åº¦ ===
è¾“å…¥æ¢¯åº¦å½¢çŠ¶: torch.Size([2, 2, 6, 6])
è¾“å…¥æ¢¯åº¦å¹³å‡å€¼: 0.150234
è¾“å…¥æ¢¯åº¦æ ‡å‡†å·®: 0.489123
âœ“ è¾“å…¥æ¢¯åº¦æµ‹è¯•é€šè¿‡

=== æµ‹è¯•æ•°å€¼æ¢¯åº¦ ===
æœ€å¤§ç›¸å¯¹è¯¯å·®: 0.000023
å¹³å‡ç›¸å¯¹è¯¯å·®: 0.000008
âœ“ æ•°å€¼æ¢¯åº¦æµ‹è¯•é€šè¿‡

======================================================================
âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼
======================================================================
```

### ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

#### é€Ÿåº¦å¯¹æ¯”

```python
import torch
import time
from dynamic_dilation_unfold import dynamic_dilation_unfold

def benchmark(func, *args, warmup=10, iterations=100):
    # é¢„çƒ­
    for _ in range(warmup):
        func(*args)
    
    torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(iterations):
        func(*args)
    
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    return elapsed / iterations * 1000  # ms

# è®¾ç½®
B, C, H, W = 4, 64, 128, 128
x = torch.randn(B, C, H, W).cuda()
dilation_map = torch.ones(B, 1, H, W).cuda()

# åŸºå‡†æµ‹è¯•
time_dynamic = benchmark(lambda: dynamic_dilation_unfold(x, 3, dilation_map, padding=1))

print(f"åŠ¨æ€æ‰©å¼ Unfold: {time_dynamic:.2f} ms")
print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
```

#### å†…å­˜ä½¿ç”¨

```python
import torch
from dynamic_dilation_unfold import dynamic_dilation_unfold

torch.cuda.reset_peak_memory_stats()

x = torch.randn(4, 64, 128, 128).cuda()
dilation_map = torch.ones(4, 1, 128, 128).cuda()

output = dynamic_dilation_unfold(x, kernel_size=3, dilation_map=dilation_map, padding=1)

memory_mb = torch.cuda.max_memory_allocated() / 1024 / 1024
print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {memory_mb:.2f} MB")
```

### ğŸ”¬ å·¥ä½œåŸç†

#### ç®—æ³•æ¦‚è¿°

å¯¹äºæ¯ä¸ªè¾“å‡ºä½ç½®`(b, c, h_out, w_out)`ï¼š

1. **è¯»å–æ‰©å¼ ç‡**ï¼šè·å–`d = dilation_map[b, 0, h_out, w_out]`

2. **è®¡ç®—é‡‡æ ·ä½ç½®**ï¼šå¯¹äºæ¯ä¸ªkernelä½ç½®`(kh, kw)`ï¼š
   ```python
   h_in = h_out * stride_h - pad_h + (kh - (K-1)/2) * base_dilation * d
   w_in = w_out * stride_w - pad_w + (kw - (K-1)/2) * base_dilation * d
   ```

3. **åŒçº¿æ€§æ’å€¼**ï¼šåœ¨`(h_in, w_in)`ä½ç½®ä½¿ç”¨åŒçº¿æ€§æ’å€¼é‡‡æ ·è¾“å…¥

4. **å­˜å‚¨ç»“æœ**ï¼šå°†é‡‡æ ·å€¼æ”¾å…¥è¾“å‡ºå¼ é‡

#### åå‘ä¼ æ’­

**å¯¹è¾“å…¥çš„æ¢¯åº¦ï¼š**
- å°†è¾“å‡ºæ¢¯åº¦åˆ†é…åˆ°åŒçº¿æ€§æ’å€¼ä½¿ç”¨çš„4ä¸ªé‚»è¿‘åƒç´ 
- ä½¿ç”¨åŸå­æ“ä½œä¿è¯çº¿ç¨‹å®‰å…¨çš„ç´¯ç§¯

**å¯¹æ‰©å¼ å›¾çš„æ¢¯åº¦ï¼š**
- è®¡ç®—é‡‡æ ·ä½ç½®å¦‚ä½•éšæ‰©å¼ ç‡å˜åŒ–
- é€šè¿‡åŒçº¿æ€§æ’å€¼åº”ç”¨é“¾å¼æ³•åˆ™
- åœ¨æ‰€æœ‰kernelä½ç½®å’Œé€šé“ä¸Šç´¯ç§¯

#### ç¤ºæ„å›¾

```
æ ‡å‡†Unfold (æ‰©å¼ ç‡=1):              åŠ¨æ€æ‰©å¼  (dilation_map):
                                       
    [Â·][Â·][Â·]                              [Â·]    [Â·]    [Â·]
    [Â·][X][Â·]      kernel_size=3           
    [Â·][Â·][Â·]                              [Â·]    [X]    [Â·]      æ‰©å¼ ç‡=2.0
                                           
                                           [Â·]    [Â·]    [Â·]


    [Â·][Â·][Â·]                              [Â·]      [Â·]      [Â·]
    [Â·][X][Â·]      kernel_size=3           
    [Â·][Â·][Â·]                              [Â·]      [X]      [Â·]  æ‰©å¼ ç‡=3.0
                                           
                                           [Â·]      [Â·]      [Â·]
```

### ğŸ¤” å¸¸è§é—®é¢˜

**Q: ä¸æ ‡å‡†`F.unfold`æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**

A: æ ‡å‡†`F.unfold`å¯¹æ‰€æœ‰ä½ç½®ä½¿ç”¨å›ºå®šçš„æ‰©å¼ ç‡ã€‚åŠ¨æ€æ‰©å¼ Unfoldå…è®¸æ¯ä¸ªç©ºé—´ä½ç½®å…·æœ‰ç”±`dilation_map`æŒ‡å®šçš„è‡ªå·±çš„æ‰©å¼ ç‡ã€‚

**Q: å¯ä»¥ä¸å·ç§¯å±‚ä¸€èµ·ä½¿ç”¨å—ï¼Ÿ**

A: è¿™ä¸“é—¨ç”¨äºunfoldæ“ä½œã€‚å¯¹äºåŠ¨æ€å·ç§¯ï¼Œè€ƒè™‘ä½¿ç”¨Deformable Convolution (DCNv2)ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥å°†æ­¤unfoldä¸æ‰‹åŠ¨çŸ©é˜µä¹˜æ³•ç»“åˆä»¥å®ç°ç±»ä¼¼æ•ˆæœã€‚

**Q: å¦‚æœæ‰©å¼ ç‡ä¸º0ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**

A: å½“æ‰©å¼ ç‡ä¸º0æ—¶ï¼Œæ‰€æœ‰kernelä½ç½®éƒ½ä»ä¸­å¿ƒç‚¹é‡‡æ ·ã€‚è¿™åˆ›å»ºäº†ä¸€ä¸ª"å¡Œé™·"çš„æ„Ÿå—é‡ï¼Œåœ¨æŸäº›åº”ç”¨ä¸­å¯èƒ½å¾ˆæœ‰ç”¨ã€‚

**Q: è¿™æ˜¯å¯å¾®åˆ†çš„å—ï¼Ÿ**

A: æ˜¯çš„ï¼è¾“å…¥å’Œdilation_mapéƒ½é€šè¿‡åŒçº¿æ€§æ’å€¼å…·æœ‰å®Œæ•´çš„æ¢¯åº¦æ”¯æŒã€‚

**Q: å¯ä»¥ä½¿ç”¨å°æ•°æ‰©å¼ å€¼å—ï¼Ÿ**

A: å¯ä»¥ï¼dilation_mapæ”¯æŒä»»ä½•éè´Ÿæµ®ç‚¹å€¼ï¼Œå®ç°å¯¹æ„Ÿå—é‡å¤§å°çš„å¹³æ»‘ã€è¿ç»­æ§åˆ¶ã€‚

**Q: è®¡ç®—å¼€é”€æ˜¯å¤šå°‘ï¼Ÿ**

A: ä¸»è¦å¼€é”€æ¥è‡ªåŒçº¿æ€§æ’å€¼ï¼ˆæ¯ä¸ªkernelä½ç½®4æ¬¡é‡‡æ ·ï¼‰å’Œåå‘ä¼ æ’­ä¸­çš„åŸå­æ“ä½œã€‚å¯¹äºç±»ä¼¼é…ç½®ï¼Œé€šå¸¸æ¯”æ ‡å‡†unfoldæ…¢2-3å€ã€‚

**Q: æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒå—ï¼Ÿ**

A: æ”¯æŒï¼è¯¥å®ç°æ”¯æŒFP16ï¼ˆåŠç²¾åº¦ï¼‰ã€FP32ï¼ˆå•ç²¾åº¦ï¼‰å’ŒFP64ï¼ˆåŒç²¾åº¦ï¼‰ã€‚

### ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚

### ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®å—ä»¥ä¸‹é¡¹ç›®å¯å‘ï¼š
- [Deformable Convolution V2](https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch) by Chengdazhi
- [DCNv2](https://github.com/CharlesShang/DCNv2) by CharlesShang

---
